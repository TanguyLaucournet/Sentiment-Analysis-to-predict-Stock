# -*- coding: utf-8 -*-
"""SnTweet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XUrWGI_oSJhRzy1MSwWryMrguVlk3PUL

5) Webscrapping Twitter banquier centraux (U.E ou US) pour prédiction tendance up ou down du SX5E (si Europe) ou NYSE (si US)
- L'idée est de partir des twitts #BCE ou #fed par exemple ou les twit des banquiers centraux (Christine Lagarde, François Villeroy De Gallhau par ex...) et d'essayer d'estimer via ces twit quel sera la tendance (up ou down) du SX5E dans le futur.
- Par exemple vous pouvez prendre le cours du SX5E et des différents actions qui le constituent + ajouter le sentiment dégagé par les twit de Christine Lagarde comme variable explicative pour prédire si au mois n+1 le cours du sx5e va augmenter ou baisser.
- Vous êtes libres de prendre la profondeur que vous jugerez pertinentes (profondeur en retwitt et en nombre de jours avant le mois n+1
"""

!pip install snscrape
!pip install langdetect
!pip install nltk

import snscrape.modules.twitter as sntwitter
import pandas as pd
import pandas_datareader as web
import numpy as np
from langdetect import detect
import re
import matplotlib.pyplot as plt
import nltk

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

tweets_list= []
for i,tweet in enumerate(sntwitter.TwitterSearchScraper('SX5E since:2015-06-01 until:2021-01-01').get_items()):
  if i>5000:
    break
  tweets_list.append([tweet.date, tweet.content, tweet.username])
tweets_df = pd.DataFrame(tweets_list, columns=['Date', 'Text', 'Username'])
tweets_df['Date'] = pd.to_datetime(tweets_df['Date']).dt.date

tweets_df.to_csv('SnTweet.csv')

def deleteNonEnglishTweet(text):
  lang = detect(text)
  if lang != 'en' or "SX5E" not in text:
    text=''
  
  else:
    return (text)

tweets_df['Text'] = tweets_df['Text'].apply(lambda x: deleteNonEnglishTweet(x))
# replace epmty string with np.nan to drop them easily
tweets_df['Text'].replace('', np.nan, inplace=True)
tweets_df.dropna(subset=['Text'], inplace=True)

tweets_df['Username'].replace('IC_Markets', np.nan, inplace=True)
tweets_df.dropna(subset=['Username'], inplace=True)

tweets_df['Text'] = tweets_df['Text'].apply(lambda x: x.replace('#SX5E', 'SX5E'))

web_address = re.compile(r"(?i)http(s):\/\/[a-z0-9.~_\-\/]+")
sx5e = re.compile(r"(?i)@SX5E(?=\b)")
user = re.compile(r"(?i)@[a-z0-9_]+")
hashtag = re.compile(r"(?i)#[A-Za-z0-9]+")

# we then use the sub method to replace anything matching
tweets_df['Text'] = tweets_df['Text'].apply(lambda x : web_address.sub('', x))
tweets_df['Text'] = tweets_df['Text'].apply(lambda x : user.sub('', x))
tweets_df['Text'] = tweets_df['Text'].apply(lambda x : sx5e.sub('SX5E', x))
tweets_df['Text'] = tweets_df['Text'].apply(lambda x : hashtag.sub('', x))

vader = SentimentIntensityAnalyzer()
tweets_df['compound'] = tweets_df['Text'].apply(lambda x: vader.polarity_scores(x)['compound'])

mean_df = pd.DataFrame(columns=['Date', 'sentiment'])
plt.figure(figsize=(16,8))
mean_df = tweets_df.groupby(['Date']).mean()
mean_df = mean_df.xs('compound', axis="columns")
mean_df.plot()
plt.show()

price_df = web.DataReader('^STOXX50E', data_source='yahoo', start='2015-06-01', end='2021-01-01')

plt.figure(figsize=(16,8))
plt.title('Closing price history')
plt.plot(price_df['Close'])
plt.xlabel('Date', fontsize = 18)
plt.xlabel('Price', fontsize = 18)

#add sentiment to main df
price_df['sentiment']=mean_df
price_df['sentiment'].fillna(0, inplace = True)
#add label to main df
label=[]
for i in range(len(price_df.index)):
  if price_df['Close'][i]>price_df['Open'][i]:
    label.append(1)
  else:
    label.append(0)
price_df['label'] = label

price_df.to_csv('PticeSX5E.csv')

keep_col = ['Open','High','Low','Volume','sentiment','label']
df = price_df[keep_col]



#Create feature data set
X = df
X = np.array(X.drop(['label'],1))

#Create target data set
y = np.array(df['label'])

#Split data 80% training 20% test
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0 )

#Create the model 
model = LinearDiscriminantAnalysis().fit(x_train,y_train)

# Show prediction
predictions = model.predict(x_test)
predictions

y_test

# Show metrics
print(classification_report(y_test, predictions))